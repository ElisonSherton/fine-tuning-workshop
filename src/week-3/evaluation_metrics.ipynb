{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3fc6c4-5f04-43a1-9b81-59b8635fc1c6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To complete the following guide you will need to install the following packages:\n",
    "- fireworks-ai\n",
    "- numpy\n",
    "- pandas\n",
    "- pronouncing\n",
    "- requests\n",
    "- sentence-transformers\n",
    "- transformers\n",
    "\n",
    "You will also need:\n",
    "\n",
    "- Fireworks account (https://fireworks.ai/)\n",
    "- Fireworks API key\n",
    "- The firectl command-line interface (https://docs.fireworks.ai/tools-sdks/firectl/firectl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "618062b8-6854-44cb-8d52-5ba675d6d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fireworks-ai numpy pandas pronouncing requests sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acdd56c2-87a7-419b-8c83-d65ee7e752dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from fireworks.client import Fireworks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pronouncing\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "embeddings_model = SentenceTransformer('Alibaba-NLP/gte-base-en-v1.5', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40902fd3-a249-4bc6-ab64-aea10920a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sign-in to your Fireworks account\n",
    "!firectl signin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a85cbf23-f79c-4f87-87e0-aff1ba92607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the FIREWORKS_API_KEY environment variable set to your account's key!\n",
    "# os.environ['FIREWORKS_API_KEY'] = 'XXX'\n",
    "\n",
    "client = Fireworks()\n",
    "\n",
    "# Replace the line below with your Fireworks account id\n",
    "account_id = 'XXX'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9154aee-9c43-407d-a143-5faf748be8e0",
   "metadata": {},
   "source": [
    "## Problem Definition: Poem Generation\n",
    "\n",
    "*Note: The poem topics used in this example were synthetically generated by Claude 3 Opus*\n",
    "\n",
    "LLMs are capable of performing creative writing tasks. However, assessing the quality of a creative writing task, such as poetry generation, is highly subjective.\n",
    "\n",
    "### Task\n",
    "We will create an evaluation framework to assess the quality of poetry generated by an LLM. We will then fine-tune a model using the knowledge distillation method (i.e. fine-tuning a smaller model (\"student\") using output from a larger model (\"teacher\")), and assess the improvement with our evaluation framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3242e9ff-dd75-4d8d-aa90-bc7f00db0bb0",
   "metadata": {},
   "source": [
    "#### Data\n",
    "\n",
    "The data can be found in the week-3 `data` folder.\n",
    "\n",
    "We will use the following datasets:\n",
    "- `./data/training_poem_topics.csv`\n",
    "- `./data/test_poem_topics.csv`\n",
    "\n",
    "Each of those datasets consists of 100 unique poem topics. Our first step is to generate a poem for each of these topics using a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ee8f3b7-69ad-46aa-8e5d-1392008d7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a csv file with a list of topics, generates a poem for each topic\n",
    "system_message = 'You are a professional poet. Write a unique and original contemporary poem about the topic suggested by the user. Your response should contain ONLY the content of the poem.'\n",
    "def generate_poems(model, csv_file):\n",
    "    responses = list()\n",
    "    df = pd.read_csv(csv_file)\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "              {\"role\": \"system\", \"content\": system_message},\n",
    "              {\"role\": \"user\", \"content\": row[1]['topic']}\n",
    "            ],\n",
    "        )\n",
    "        response = response.choices[0].message.content\n",
    "        responses.append(response)   \n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7697812c-f6d7-4196-b52f-542a5d0208ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates poems for each of the 100 test topics using the base 8B model\n",
    "llama_8b_poems = generate_poems('accounts/fireworks/models/llama-v3-8b-instruct', 'data/test_poem_topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0200bf1-e575-4430-bfba-c9516bb37368",
   "metadata": {},
   "source": [
    "### Heuristic Evaluation\n",
    "In class, we discussed using a heuristic-based evaluation approach when the quality of results is subjective. This method involves creating heuristics that align with your desired assessment criteria and evaluating the results based on these metrics.\n",
    "\n",
    "For this exercise, I've developed the following heuristics to assess our poems:\n",
    "\n",
    "- Average length (number of characters)\n",
    "- Rhyming percentage (average percentage of stanzas that rhyme)\n",
    "- Positive sentiment percentage (percentage of poems with positive sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e1bf2fa-b4e1-4219-9eb6-64a61aa62719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate poems based on their average length (# of characters)\n",
    "def calculate_avg_length(poems):\n",
    "    return int(np.mean([len(poem) for poem in poems]))\n",
    "\n",
    "# Evaluate poems based on the pct of stanzas that contain a rhyme\n",
    "def calculate_rhyming_fct(poem):\n",
    "    stanzas = poem.split('\\n\\n')\n",
    "    stanzas = [stanza for stanza in stanzas if len(stanza.split('\\n')) > 1]\n",
    "    \n",
    "    num_rhyming_stanzas = 0\n",
    "    for stanza in stanzas:\n",
    "        lines = stanza.split('\\n')\n",
    "        end_words = [line.split(' ')[-1].strip('.?!\"\\',') for line in lines]\n",
    "        found_rhyme = False\n",
    "        for i in range(len(end_words)):\n",
    "            for j in range(i + 1, len(end_words)):\n",
    "                found_rhyme = True if found_rhyme or (end_words[j] in pronouncing.rhymes(end_words[i])) else False\n",
    "                \n",
    "        if found_rhyme:\n",
    "            num_rhyming_stanzas += 1\n",
    "            \n",
    "    return num_rhyming_stanzas / len(stanzas)\n",
    "\n",
    "# Evaluate poems based on how often they have a positive sentiment\n",
    "def has_positive_sentiment(poem):\n",
    "    sentiment = sentiment_pipeline(poem)[0]\n",
    "    return True if sentiment['label'] == 'POSITIVE' else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf9e076a-ec83-4f8b-ba98-97f9f79cde71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic Evaluation\n",
      "Average Length: 974\n",
      "Rhyming Pct: 93%\n",
      "Positive Sentiment: 83%\n"
     ]
    }
   ],
   "source": [
    "# Calculate heuristics of the poems generated by our base model\n",
    "print(\"Heuristic Evaluation\")\n",
    "print(f'Average Length: {calculate_avg_length(llama_8b_poems)}')\n",
    "print(f\"Rhyming Pct: {int(100 * np.mean([calculate_rhyming_fct(poem) for poem in llama_8b_poems]))}%\")\n",
    "print(f\"Positive Sentiment: {int(100 * np.mean([has_positive_sentiment(poem) for poem in llama_8b_poems]))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc47a64-2f14-4398-bd94-7941eaa73432",
   "metadata": {},
   "source": [
    "### LLM as a Judge\n",
    "Another evaluation method we discussed in class is using an LLM as a judge. This approach involves employing a high-quality LLM to assess the quality of the generated results. This method is effective because LLMs are often better at evaluating content than generating it.\n",
    "\n",
    "To implement this method, you need to create a scoring rubric (i.e., \"constitution\") to guide the LLM in evaluating the results. The LLM will use this rubric to score each poem on a scale from 0 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b307b1a9-cb00-488e-abc0-ff1cd808c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we evaluate poems using our scoring rubric (i.e. \"constitution\")\n",
    "poem_guidelines = \"\"\"- Is the poem original?\n",
    "- Does the poem contain beauty, power, education or entertainment?\n",
    "- is the message of the poem clear? Is it a good message, or is it of little value to anyone?\n",
    "- Is the poem clear in its expression? Does it maintain coherence throughout?\n",
    "- If the poem is written in rhyming verse, then it should be rated according to how well the rhymes fit, not only with each other, but with the flow and the intended nuance of meaning the verse demands.\n",
    "- What form does the poem take? Is it a sonnet, free verse, haiku, etc.? How does the form contribute to the poem's impact?\n",
    "- Does the poet us the best possible choice of words in the poem? A person can ball, cry, sob, whimper, and shed tears, but which term would best fit the mood the poet is trying to convey?\"\"\"\n",
    "\n",
    "poem_evaluation_rubric = f'''You are professional poet responsible for assessing the quality of AI generated poems.\n",
    "\n",
    "Score each poem on a scale of 0 to 10, where 10 represents the best possible poem.\n",
    "\n",
    "Scoring Guidelines:\n",
    "{poem_guidelines}\n",
    "\n",
    "Think through your reasoning step-by-step and explain your reasoning. Steps for judging a poem:\n",
    "1. Read the Poem Multiple Times: Read it aloud and silently to capture both the meaning and the sound.\n",
    "2. Take Notes: Jot down initial impressions, notable phrases, and any questions that arise.\n",
    "3. Analyze the Elements: Break down the poem into its components (content, structure, language, sound).\n",
    "4. Reflect on Your Experience: Consider your emotional response and personal connection to the poem.\n",
    "\n",
    "The last line in your response MUST be a json object {{\"score\": XXX}}, where XXX is the score you are giving the response.'''\n",
    "\n",
    "def evaluate_poems(poems, evaluation_model):\n",
    "    scores = list()\n",
    "    for poem in poems:\n",
    "        response = client.chat.completions.create(\n",
    "            model=evaluation_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": poem_evaluation_rubric},\n",
    "                {\"role\": \"user\", \"content\": poem}\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        try: \n",
    "            response = response.choices[0].message.content\n",
    "            score = int(json.loads(response.split('\\n')[-1])['score'])  \n",
    "            scores.append(score)\n",
    "        except json.JSONDecodeError as jde:\n",
    "            continue\n",
    "        \n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "106d1267-9c2d-4ddf-9847-96712962ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg LLM Judge Score: 8.12\n"
     ]
    }
   ],
   "source": [
    "# We score the poems using our judge model\n",
    "llm_judge_model = 'accounts/fireworks/models/llama-v3-70b-instruct'\n",
    "llama_8b_avg_score = evaluate_poems(llama_8b_poems, llm_judge_model)\n",
    "\n",
    "print(f'Avg LLM Judge Score: {round(llama_8b_avg_score, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca20b29-365d-4e66-be11-cbdc4556ce7a",
   "metadata": {},
   "source": [
    "### Knowledge Distillation\n",
    "One approach to generating data for a fine-tuned model is knowledge distillation. This technique involves transferring knowledge from a large model to a smaller one. It entails generating responses relevant to your use case using the larger model and then using these responses to create a fine-tuning dataset. The smaller model is then fine-tuned on this dataset. In this example, we will use responses from a 70B model to fine-tune an 8B model. We will then use our evaluation framework to assess the quality of our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20470295-d4b7-4086-bee5-c5f190cfe855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we generate poems for 100 different topics than the ones we are using for our test set.\n",
    "llama_70b_training_poems = generate_poems('accounts/fireworks/models/llama-v3p1-70b-instruct', 'data/training_poem_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e9cb57c-8342-43dd-be96-8c0cfe233e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the improved poems to fireworks as our fine-tuning dataset\n",
    "def formt_poem_for_fireworks(topic, poem):\n",
    "    return {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_message}, \n",
    "        {\"role\": \"user\", \"content\": topic}, \n",
    "        {\"role\": \"assistant\", \"content\": poem}\n",
    "    ]}\n",
    "\n",
    "topics = pd.read_csv('data/training_poem_topics.csv')['topic'].tolist()\n",
    "json_objs = list()\n",
    "for i, poem in enumerate(llama_70b_training_poems):\n",
    "    msg = {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_message}, \n",
    "        {\"role\": \"user\", \"content\": topics[i]}, \n",
    "        {\"role\": \"assistant\", \"content\": poem}\n",
    "    ]}    \n",
    "    json_objs.append(msg)\n",
    "\n",
    "dataset_file_name = 'poem_training_data.jsonl'\n",
    "dataset_id = 'poem-data-v1'\n",
    "with open(dataset_file_name, 'w') as f:\n",
    "    for obj in json_objs:\n",
    "        json.dump(obj, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "532ba7c4-d8a3-44fd-99da-013410862a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload our dataset to fireworks\n",
    "!firectl create dataset {dataset_id} {dataset_file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "759f5765-bbc1-419d-b822-ebed16fb28da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fine-tuning job\n",
    "!firectl create fine-tuning-job --settings-file poem_generation_fine_tuning_config.yaml --display-name poem-generation-v1 --dataset {dataset_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f544dafc-10a8-4f2a-92af-9e0f13e20b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_id = 'd6e172f463a44907942f0c885af2e192' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44b395fb-86ff-4c5b-ba23-e5c41b122982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the State of the fine-tuning job is listed as COMPLETED (~10-20 minutes)\n",
    "!firectl get fine-tuning-job {ft_model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08b52e9f-8948-42d0-9d07-7d5d8515311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy our fine-tuned model\n",
    "!firectl deploy {ft_model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "303f57fe-825d-4d44-ad07-45f1cdeb2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the the Deploymed Model Refs lists the state of the models as \"DEPLOYED\" (~5-20 minutes).\n",
    "!firectl get model {ft_model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8cdea9fb-78cc-4293-ac5a-6f5597327d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate poems on the test set using our fine-tuned model\n",
    "ft_poems = generate_poems(f'accounts/{account_id}/models/{ft_model_id}', 'data/test_poem_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9cbb3fd-1bf0-4321-b511-fbc9e6bbf0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic Evaluation\n",
      "Average Length: 994\n",
      "Rhyming Pct: 91%\n",
      "Positive Sentiment: 88%\n"
     ]
    }
   ],
   "source": [
    "# Calculate heuristics of our fine-tuned poems\n",
    "print(\"Heuristic Evaluation\")\n",
    "print(f'Average Length: {calculate_avg_length(ft_poems)}')\n",
    "print(f\"Rhyming Pct: {int(100 * np.mean([calculate_rhyming_fct(poem) for poem in ft_poems]))}%\")\n",
    "print(f\"Positive Sentiment: {int(100 * np.mean([has_positive_sentiment(poem) for poem in ft_poems]))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd47aa0d-6d32-4d51-83e8-ba7a77b39694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg LLM Judge Score: 8.21\n"
     ]
    }
   ],
   "source": [
    "# Use the LLM to evaluate our fine-tuned model\n",
    "ft_avg_score = evaluate_poems(ft_poems, 'accounts/fireworks/models/llama-v3-70b-instruct')\n",
    "print(f\"Avg LLM Judge Score: {round(ft_avg_score , 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c4f7d0b-050c-4812-b07a-cbcadd6007c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undeploy the fine-tuned model (does not cost anything extra, but Fireworks may limit your number of deployed models).\n",
    "!firectl undeploy {ft_model_id}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
