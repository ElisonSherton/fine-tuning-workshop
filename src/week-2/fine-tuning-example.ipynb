{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b56a030b-9e82-43cc-8cde-ab20da4280a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "from datasets import load_dataset\n",
    "from fireworks.client import Fireworks\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a5636ee-8032-4f24-b69a-c90414078d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Make sure you have the FIREWORKS_API_KEY env var set to your account's key!\n",
    "account_id = 'sdkramer10-5e98cb'\n",
    "client = Fireworks()\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923e9c39-8d84-40c9-8827-b301f8d7373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a datset containing human preferences from the chatbot arena. When a human types a message, they are sent responses from two\n",
    "# different chatbots. The human then votes on which response they prefer. Throughout this course, I am going to fine-tune a model to predict\n",
    "# human chatbot preferences. For this week's assignment, I will perform prompt engineering to get a baseline of how well llama3-8b-instruct\n",
    "# performs at this task before performing fine-tuning\n",
    "# For more details on the dataset: https://huggingface.co/datasets/lmsys/chatbot_arena_conversations\n",
    "dataset = load_dataset(\"lmsys/chatbot_arena_conversations\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d53b80e9-1bf8-4da1-832e-6c44b781112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, I am only going to look at single-turn chats, where the user declared a winner after a single response from the bot.\n",
    "examples = [example for example in dataset if example['turn'] == 1]\n",
    "\n",
    "# The query the user sent to both bots should be exactly the same, so that we are fairly judging the responses. This should be always be\n",
    "# the case for this dataset. This line just acts as a sanity check.\n",
    "examples = [example for example in examples if example['conversation_a'][0]['content'] == example['conversation_b'][0]['content']]\n",
    "\n",
    "# We take different examples for the train/validation/test sets\n",
    "training_examples = examples[:2000]\n",
    "validation_examples = examples[-1000:]\n",
    "test_examples = examples[-2000:-1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f1f8c6a-90e7-4d32-b9b6-9de0a52c51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_msg = f'''Choose the better chatbot response between model_a and model_b.\n",
    "\n",
    "Your response MUST be ONLY the JSON object {{\"winner\": XXX}}. XXX can only equal \"model_a\", \"model_b\", \"tie\", or \"tie (bothbad)\".'''\n",
    "\n",
    "def get_user_msg(example):\n",
    "    user_query = example['conversation_a'][0]['content']\n",
    "    model_a_response = example['conversation_a'][1]['content']\n",
    "    model_b_response = example['conversation_b'][1]['content']\n",
    "    user_msg = f\"\"\"user query: {user_query}\n",
    "\n",
    "model_a response: {model_a_response}\n",
    "\n",
    "model_b response: {model_b_response}\"\"\"\n",
    "    return user_msg\n",
    "\n",
    "# Even though this is a classification task, the chat completions api from Fireworks is the most general and most well developed,\n",
    "# and performs the best\n",
    "def create_messages(example):\n",
    "    user_msg = get_user_msg(example)\n",
    "    asst_msg = json.dumps({\"winner\": example['winner']})\n",
    "\n",
    "    return {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": sys_msg}, \n",
    "        {\"role\": \"user\", \"content\": user_msg}, \n",
    "        {\"role\": \"assistant\", \"content\": asst_msg}\n",
    "    ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f2e886f-494d-4f51-8e00-72c2a18e2c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1052349\n"
     ]
    }
   ],
   "source": [
    "# Converts the training examples to the format expected by Fireworks. See https://readme.fireworks.ai/docs/fine-tuning-models#conversation\n",
    "def training_examples_to_json(examples):\n",
    "    json_objs = list()\n",
    "    for example in examples:  \n",
    "        msg = create_messages(example)\n",
    "        json_objs.append(msg)\n",
    "    \n",
    "    print(f'Total tokens: {sum([len(tokenizer.tokenize(json.dumps(obj))) for obj in json_objs])}')\n",
    "    return json_objs\n",
    "\n",
    "training_json = training_examples_to_json(training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68de340d-f246-4243-a205-5a4ebf2b7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes the data to a file so that it can be uploaded to Fireworks\n",
    "dataset_file_name = 'chatbot_arena_training_data.jsonl'\n",
    "dataset_id = 'chatbot-arena-v3'\n",
    "\n",
    "with open(dataset_file_name, 'w') as f:\n",
    "    for obj in training_json:\n",
    "        json.dump(obj, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6a68299-9751-4263-ae87-1a9a531caf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.98 MiB / 3.98 MiB [--------------------------------] 100.00% 2.51 MiB p/s 1.8s\n"
     ]
    }
   ],
   "source": [
    "# Follow instructions here to first install the firectil CLI - https://readme.fireworks.ai/docs/fine-tuning-models#installing-firectl\n",
    "# Then run this command to upload the file to Fireworks\n",
    "!firectl create dataset {dataset_id} {dataset_file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee8dd75b-d1d4-4e9c-be55-e7c7777cbb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a training job with the default hyperparameters\n",
    "# Uncomment out to run (prints my api key to stdout, so commenting it out for the demo).\n",
    "# !firectl create fine-tuning-job --settings-file chatbot_arena_training_v1.yaml --display-name chatbot-arena-v1 --dataset {dataset_id} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d541454-c4b0-4a4c-b058-4cb19cb48542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a training job with the increased rank, learning rate, and epochs\n",
    "# Uncomment out to run (prints my api key to stdout, so commenting it out for the demo).\n",
    "# !firectl create fine-tuning-job --settings-file chatbot_arena_training_v2.yaml --display-name chatbot-arena-v2 --dataset {dataset_id} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8565ac9-eb20-4070-8c7c-7796d199f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1 is the id of the training job with default hyperparameters, v2 is with the increased settings\n",
    "model_v1_id = 'cc8324868ff04936855cffb392dba3b8'\n",
    "model_v2_id = '4fe290a74b72458cafe0c9d8881e5d37'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "498a82de-3e66-47ec-bab4-b951ef86c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the fine-tuning jobs have finished running.\n",
    "# Uncomment out to run (prints my api key to stdout, so commenting it out for the demo).\n",
    "# !firectl get fine-tuning-job {model_v1_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6d45aab-a44f-4442-8e84-e14751b688d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Deploy the first model to a serverless endpoint\n",
    "!firectl deploy {model_v1_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9580299-573a-4727-8f7b-3d0f068cebcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Deploy the second model to a serverless endpoint\n",
    "!firectl deploy {model_v2_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f081c2cc-d5a5-4763-8902-5833522c7fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                              CREATE TIME          KIND           CHAT  PUBLIC  STATE     STATUS MESSAGE\n",
      "4fe290a74b72458cafe0c9d8881e5d37  2024-06-18 19:36:34  HF_PEFT_ADDON  true  false   DEPLOYED  \n",
      "cc8324868ff04936855cffb392dba3b8  2024-06-18 19:31:50  HF_PEFT_ADDON  true  false   DEPLOYED  \n",
      "\n",
      "Total size: 2\n"
     ]
    }
   ],
   "source": [
    "# Wait until the models are in the \"DEPLOYED\" state\n",
    "!firectl list models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cbba9937-f451-4677-98aa-9f489f0927d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(examples, model_id):\n",
    "    winners = list()\n",
    "    \n",
    "    for i, example in enumerate(examples):    \n",
    "        user_msg = get_user_msg(example)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_msg},\n",
    "                {\"role\": \"user\", \"content\": user_msg},\n",
    "            ],\n",
    "            # setting temperature to 0 for this use case, so that responses are as deterministic as possible\n",
    "            temperature=0, \n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "    \n",
    "        try:\n",
    "            winner = json.loads(content.split('\\n')[-1])[\"winner\"]\n",
    "            winners.append((i, winner))\n",
    "        except:\n",
    "            print(f\"Failed to parse JSON for example {i}.\")\n",
    "\n",
    "    num_correct = sum([1 if winner[1] == examples[winner[0]]['winner'] else 0 for winner in winners])\n",
    "    return winners, num_correct\n",
    "        \n",
    "num_to_eval = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e18e6cb6-cf83-4393-a804-a13360794b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Correct: 302\n",
      "Validation Set Correct: 251\n"
     ]
    }
   ],
   "source": [
    "# Determine how the base model without any fine-tuning performs\n",
    "model_id = 'accounts/fireworks/models/llama-v3-8b-instruct'\n",
    "\n",
    "train_results, train_num_correct = get_results(training_examples[:num_to_eval], model_id)\n",
    "print(f'Training Set Correct: {train_num_correct}')\n",
    "\n",
    "validation_results, validation_num_correct = get_results(validation_examples[:num_to_eval], model_id)\n",
    "print(f'Validation Set Correct: {validation_num_correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "256d7c65-4050-43e0-881d-58a1ad398de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse JSON for example 127.\n",
      "Training Set Correct: 336\n",
      "Validation Set Correct: 281\n"
     ]
    }
   ],
   "source": [
    "# Determine how the fine-tuned model performs with the default fine-tuning params\n",
    "model_id = f'accounts/{account_id}/models/{model_v1_id}'\n",
    "\n",
    "train_results, train_num_correct = get_results(training_examples[:num_to_eval], model_id)\n",
    "print(f'Training Set Correct: {train_num_correct}')\n",
    "\n",
    "validation_results, validation_num_correct = get_results(validation_examples[:num_to_eval], model_id)\n",
    "print(f'Validation Set Correct: {validation_num_correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "198b6a3a-422e-4157-b5b2-b00f9ddcb106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Correct: 369\n",
      "Validation Set Correct: 279\n"
     ]
    }
   ],
   "source": [
    "# Determine how the base model performs with the increases rank, epochs, and learning rate\n",
    "model_id = f'accounts/{account_id}/models/{model_v2_id}'\n",
    "\n",
    "train_results, train_num_correct = get_results(training_examples[:num_to_eval], model_id)\n",
    "print(f'Training Set Correct: {train_num_correct}')\n",
    "\n",
    "validation_results, validation_num_correct = get_results(validation_examples[:num_to_eval], model_id)\n",
    "print(f'Validation Set Correct: {validation_num_correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86c759ab-f84e-44bb-b621-86550519c9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Undeploy the models (shouldn't cost anything extra to leave deployed, but the fireworks documentation is conflicting on this point).\n",
    "!firectl undeploy {model_v1_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b09d5f87-0bcb-4078-a6ff-2a02c8ccc48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Undeploy the models (shouldn't cost anything extra to leave deployed, but the fireworks documentation is conflicting on this point).\n",
    "!firectl undeploy {model_v2_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb00690d-da5d-43ae-a48d-50e8df45a927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                              CREATE TIME          KIND           CHAT  PUBLIC  STATE       STATUS MESSAGE\n",
      "4fe290a74b72458cafe0c9d8881e5d37  2024-06-18 19:36:34  HF_PEFT_ADDON  true  false   UNDEPLOYED  \n",
      "cc8324868ff04936855cffb392dba3b8  2024-06-18 19:31:50  HF_PEFT_ADDON  true  false   UNDEPLOYED  \n",
      "\n",
      "Total size: 2\n"
     ]
    }
   ],
   "source": [
    "# Double check that the model is now undeployed\n",
    "!firectl list models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
