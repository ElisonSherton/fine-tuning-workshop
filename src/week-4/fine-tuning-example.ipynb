{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "482db629-f787-44fd-a907-03cc915f1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets==2.20.0 fireworks-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b56a030b-9e82-43cc-8cde-ab20da4280a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import fireworks.client as fc\n",
    "from fireworks.client import Fireworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5636ee-8032-4f24-b69a-c90414078d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the line below to your account id\n",
    "account_id = 'sdkramer10-5e98cb'\n",
    "\n",
    "# Uncomment the line below and set the value to your account's API key. Alternatively, set the FIREWORKS_API_KEY environment variable.\n",
    "# fc.api_key = '<API KEY>'\n",
    "\n",
    "client = Fireworks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "923e9c39-8d84-40c9-8827-b301f8d7373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a datset containing human preferences from the chatbot arena. When a human types a message, they are sent responses from two\n",
    "# different chatbots. The human then votes on which response they prefer. Throughout this course, I am going to fine-tune a model to predict\n",
    "# human chatbot preferences. For this week's assignment, I will perform prompt engineering to get a baseline of how well llama3-8b-instruct\n",
    "# performs at this task before performing fine-tuning\n",
    "# For more details on the dataset: https://huggingface.co/datasets/lmsys/chatbot_arena_conversations\n",
    "dataset = load_from_disk(\"./chatbot_arena_dataset/\")['train'].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d53b80e9-1bf8-4da1-832e-6c44b781112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, I am only going to look at single-turn chats, where the user declared a winner after a single response from the bot.\n",
    "examples = [example for example in dataset if example['turn'] == 1]\n",
    "\n",
    "# The query the user sent to both bots should be exactly the same, so that we are fairly judging the responses. This should be always be\n",
    "# the case for this dataset. This line just acts as a sanity check.\n",
    "examples = [example for example in examples if example['conversation_a'][0]['content'] == example['conversation_b'][0]['content']]\n",
    "\n",
    "# We take different examples for the train/validation/test sets\n",
    "training_examples = examples[:2000]\n",
    "validation_examples = examples[-1000:]\n",
    "test_examples = examples[-2000:-1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "553708c4-8184-4e91-ae27-b53d495c6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_msg = f'''Choose the better chatbot response between model_a and model_b.\n",
    "\n",
    "Your response MUST be ONLY the JSON object {{\"winner\": XXX}}. XXX can only equal \"model_a\", \"model_b\", \"tie\", or \"tie (bothbad)\".'''\n",
    "\n",
    "# Converts the training examples to the format expected by Fireworks. See https://readme.fireworks.ai/docs/fine-tuning-models#conversation\n",
    "def get_user_msg(example):\n",
    "    user_query = example['conversation_a'][0]['content']\n",
    "    model_a_response = example['conversation_a'][1]['content']\n",
    "    model_b_response = example['conversation_b'][1]['content']\n",
    "    user_msg = f\"\"\"user query: {user_query}\n",
    "\n",
    "model_a response: {model_a_response}\n",
    "\n",
    "model_b response: {model_b_response}\"\"\"\n",
    "    return user_msg\n",
    "\n",
    "def create_messages(example):\n",
    "    user_msg = get_user_msg(example)\n",
    "    asst_msg = json.dumps({\"winner\": example['winner']})\n",
    "\n",
    "    return {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": sys_msg}, \n",
    "        {\"role\": \"user\", \"content\": user_msg}, \n",
    "        {\"role\": \"assistant\", \"content\": asst_msg}\n",
    "    ]}\n",
    "\n",
    "def training_examples_to_json(examples):\n",
    "    json_objs = list()\n",
    "    for example in examples:  \n",
    "        msg = create_messages(example)\n",
    "        json_objs.append(msg)\n",
    "    \n",
    "    return json_objs\n",
    "\n",
    "# Writes the data to a file so that it can be uploaded to Fireworks\n",
    "def dataset_to_jsonl(examples, dataset_file_name):\n",
    "\n",
    "    training_json = training_examples_to_json(training_examples)\n",
    "    \n",
    "    with open(dataset_file_name, 'w') as f:\n",
    "        for obj in training_json:\n",
    "            json.dump(obj, f)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd5b089e-d05a-4b99-8828-33d28f1ef535",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file_name = 'chatbot_arena_training_data.jsonl'\n",
    "dataset_id = 'chatbot-arena-td-v1'\n",
    "dataset_to_jsonl(training_examples, dataset_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c033fd7-b7e1-4055-b7fd-1e40a76df15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.27 MiB / 4.27 MiB [--------------------------------] 100.00% 1.94 MiB p/s 2.4s\n"
     ]
    }
   ],
   "source": [
    "# Follow instructions here to first install the firectil CLI - https://readme.fireworks.ai/docs/fine-tuning-models#installing-firectl\n",
    "# Then run this command to upload the file to Fireworks\n",
    "!firectl create dataset {dataset_id} {dataset_file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d541454-c4b0-4a4c-b058-4cb19cb48542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a training job with the increased rank, learning rate, and epochs\n",
    "# Uncomment out to run (prints my api key to stdout, so commenting it out for the demo).\n",
    "!firectl create fine-tuning-job --settings-file chatbot_arena_training_v1.yaml --display-name {dataset_id} --dataset {dataset_id} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8565ac9-eb20-4070-8c7c-7796d199f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1_id = 'db6e8954716049e9aae361d59384dd7f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "498a82de-3e66-47ec-bab4-b951ef86c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the fine-tuning jobs have finished running.\n",
    "# Uncomment out to run (prints my api key to stdout, so commenting it out for the demo).\n",
    "!firectl get fine-tuning-job {model_v1_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6d45aab-a44f-4442-8e84-e14751b688d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the first model to a serverless endpoint\n",
    "!firectl deploy {model_v1_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "28a147bc-09b1-4600-aa32-41c00586a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!firectl get model {model_v1_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbba9937-f451-4677-98aa-9f489f0927d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(examples, model_id):\n",
    "    winners = list()\n",
    "    \n",
    "    for i, example in enumerate(examples):    \n",
    "        user_msg = get_user_msg(example)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_msg},\n",
    "                {\"role\": \"user\", \"content\": user_msg},\n",
    "            ],\n",
    "            # setting temperature to 0 for this use case, so that responses are as deterministic as possible\n",
    "            temperature=0, \n",
    "        )\n",
    "        content = response.choices[0].message.content.replace(\"<|start_header_id|>\", \"\")\n",
    "    \n",
    "        try:\n",
    "            winner = json.loads(content.split('\\n')[-1])[\"winner\"]\n",
    "            winners.append((i, winner))\n",
    "        except:\n",
    "            print(f\"Failed to parse JSON for example {i}.\")\n",
    "\n",
    "    num_correct = sum([1 if winner[1] == examples[winner[0]]['winner'] else 0 for winner in winners])\n",
    "    return winners, num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "256d7c65-4050-43e0-881d-58a1ad398de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Correct: 587\n"
     ]
    }
   ],
   "source": [
    "# Determine how the fine-tuned model performs with the default fine-tuning params\n",
    "model_name = f'accounts/{account_id}/models/{model_v1_id}'\n",
    "\n",
    "validation_results, validation_num_correct = get_results(validation_examples, model_name)\n",
    "print(f'Validation Set Correct: {validation_num_correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "28ece44c-b9ae-4525-8b84-649cd2b730e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Counts: [('model_a', 431), ('model_b', 423), ('tie (bothbad)', 97), ('tie', 48), ('data', 1)]\n",
      "Actual Counts: [('model_b', 355), ('model_a', 351), ('tie (bothbad)', 186), ('tie', 108)]\n"
     ]
    }
   ],
   "source": [
    "counts = Counter([m[1] for m in validation_results])\n",
    "print(f'Predicted Counts: {counts.most_common()}')\n",
    "\n",
    "counts = Counter([v['winner'] for v in validation_examples])\n",
    "print(f'Actual Counts: {counts.most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c178cbee-1997-424b-a669-95b0ff3c055c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('English', 1786),\n",
       " ('German', 46),\n",
       " ('Spanish', 33),\n",
       " ('French', 32),\n",
       " ('Russian', 18),\n",
       " ('Portuguese', 13),\n",
       " ('Italian', 11),\n",
       " ('Dutch', 6),\n",
       " ('Polish', 5),\n",
       " ('Chinese', 5),\n",
       " ('Danish', 4),\n",
       " ('unknown', 4),\n",
       " ('Japanese', 4),\n",
       " ('Finnish', 3),\n",
       " ('Indonesian', 3),\n",
       " ('Vietnamese', 3),\n",
       " ('Hungarian', 3),\n",
       " ('Korean', 3),\n",
       " ('Latin', 2),\n",
       " ('Malay', 2),\n",
       " ('Ukrainian', 2),\n",
       " ('Slovenian', 2),\n",
       " ('Galician', 2),\n",
       " ('Scots', 1),\n",
       " ('Malagasy', 1),\n",
       " ('Persian', 1),\n",
       " ('Greek', 1),\n",
       " ('Hebrew', 1),\n",
       " ('Swedish', 1),\n",
       " ('Interlingua', 1),\n",
       " ('Bangla', 1)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = Counter([v['language'] for v in training_examples])\n",
    "counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d0015334-723a-4ebc-b3fa-9f798b069a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pct_correct(examples, results):\n",
    "    num_correct = sum([1 if winner[1] == examples[winner[0]]['winner'] else 0 for winner in results])\n",
    "    pct_correct = round(100 * num_correct / len(results), 2)\n",
    "    print(f'Num Examples: {len(results)}')\n",
    "    print(f'Num Correct: {num_correct}')\n",
    "    print(f'Pct Correct: {pct_correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d9cae781-11ba-494b-9eca-2444c5bb139a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "Num Examples: 897\n",
      "Num Correct: 525\n",
      "Pct Correct: 58.53\n",
      "\n",
      "Non-English\n",
      "Num Examples: 103\n",
      "Num Correct: 62\n",
      "Pct Correct: 60.19\n"
     ]
    }
   ],
   "source": [
    "print('English')\n",
    "english_results = [res for res in validation_results if validation_examples[res[0]]['language'] == 'English']\n",
    "calculate_pct_correct(validation_examples, english_results)\n",
    "\n",
    "print('\\nNon-English')\n",
    "non_english_results = [res for res in validation_results if validation_examples[res[0]]['language'] != 'English']\n",
    "calculate_pct_correct(validation_examples, non_english_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ceed87fd-2aa6-4efe-90a1-cc38554a6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, winner in validation_results:\n",
    "    example = validation_examples[i]\n",
    "    if example['winner'] == winner or example['winner'] != 'tie (bothbad)':\n",
    "        continue\n",
    "\n",
    "    print(f\"Example {i}: Predicted Winner: {winner}, Actual Winner: {example['winner']}\\n\")\n",
    "    user_message = example['conversation_a'][0]\n",
    "    print(f\"User: {user_message['content']}\\n\")\n",
    "    print(f\"Model A: {example['conversation_a'][1]['content']}\\n\")\n",
    "    print(f\"Model B: {example['conversation_b'][1]['content']}\\n\")\n",
    "    print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5da39a99-6a2d-4043-8d27-6732f0914da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_question_category(examples, sys_msg, model_id):\n",
    "    responses = list()\n",
    "    \n",
    "    for i, example in enumerate(examples):    \n",
    "        user_query = example['conversation_a'][0]['content']\n",
    "    \n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_msg},\n",
    "                {\"role\": \"user\", \"content\": user_query},\n",
    "            ],\n",
    "            # setting temperature to 0 for this use case, so that responses are as deterministic as possible\n",
    "            temperature=0, \n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        content = content.strip().strip('\"\\'').lower()\n",
    "        is_in_category = (content == \"true\")\n",
    "        responses.append(is_in_category)\n",
    "    \n",
    "    return responses\n",
    "\n",
    "\n",
    "math_sys_msg = f'''Determine whether the user is asking a math question.\n",
    "\n",
    "Your response MUST be ONLY the value \"true\" or \"false\" and NOTHING ELSE!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cdf24e1c-3091-447f-a10e-fdbac9bb7815",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_questions = is_in_question_category(validation_examples, math_sys_msg, 'accounts/fireworks/models/llama-v3-70b-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1fbbe1b9-822e-4227-b933-89d789b47512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math\n",
      "Num Examples: 174\n",
      "Num Correct: 99\n",
      "Pct Correct: 56.9\n",
      "\n",
      "Not Math\n",
      "Num Examples: 826\n",
      "Num Correct: 488\n",
      "Pct Correct: 59.08\n"
     ]
    }
   ],
   "source": [
    "print('Math')\n",
    "math_results = [res for res in validation_results if math_questions[res[0]]]\n",
    "calculate_pct_correct(validation_examples, math_results)\n",
    "\n",
    "print('\\nNot Math')\n",
    "non_math_coding_results = [res for res in validation_results if not math_questions[res[0]]]\n",
    "calculate_pct_correct(validation_examples, non_math_coding_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c759ab-f84e-44bb-b621-86550519c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optiona: undeploy the model (shouldn't cost anything extra to leave deployed).\n",
    "# !firectl undeploy {model_v1_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c34e2dad-02f3-48cd-8e3e-dc6cd0043b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_num_training_examples = 2000\n",
    "addtl_math_questions = is_in_question_category(\n",
    "    examples[orig_num_training_examples:orig_num_training_examples+2000],\n",
    "    math_sys_msg, \n",
    "    'accounts/fireworks/models/llama-v3-70b-instruct'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0287f19c-1a8b-427d-a920-5b72494bd8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "addtl_math_questions_v2 = is_in_question_category(\n",
    "    examples[2000:6000],\n",
    "    math_sys_msg, \n",
    "    'accounts/fireworks/models/llama-v3-70b-instruct'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f99c50ff-4294-473e-8d92-1230a2ee76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = examples[:orig_num_training_examples]\n",
    "for i, is_math in enumerate(addtl_math_questions):\n",
    "    if is_math:\n",
    "        training_examples.append(examples[orig_num_training_examples+i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "178551f8-ad16-46ea-ab3a-e40c804aa620",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file_name = 'chatbot_arena_training_data_more_math.jsonl'\n",
    "dataset_id = 'chatbot-arena-td-more-math-v2'\n",
    "dataset_to_jsonl(training_examples, dataset_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6a641ff2-799d-43b8-b854-b9bbf291c474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.20 MiB / 5.20 MiB [--------------------------------] 100.00% 1.94 MiB p/s 2.9s\n"
     ]
    }
   ],
   "source": [
    "!firectl create dataset {dataset_id} {dataset_file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cf75f917-3bbf-4784-9f6c-f4f64fd0698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!firectl create fine-tuning-job --settings-file chatbot_arena_training_v1.yaml --display-name {dataset_id} --dataset {dataset_id} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "35713f8a-8a41-4b8b-b159-a71d3f51b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2_id = '39b93365bd66462792d432b8064915c4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fc8d5de8-30c8-4e75-99dd-5e7f0dfc7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!firectl get fine-tuning-job {model_v2_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c0dcf996-68ac-4be8-a3fe-8260a62d6849",
   "metadata": {},
   "outputs": [],
   "source": [
    "!firectl deploy {model_v2_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a89645f4-2d58-4652-9ac0-c171a121883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!firectl get model {model_v2_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b232df0c-1ee0-46e8-9d77-a386a44df86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Correct: 597\n"
     ]
    }
   ],
   "source": [
    "model_name = f'accounts/{account_id}/models/{model_v2_id}'\n",
    "validation_results_v2, validation_num_correct_v2 = get_results(validation_examples, model_name)\n",
    "print(f'Validation Set Correct: {validation_num_correct_v2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "362d0553-4f10-4570-957a-975a05f5c187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math\n",
      "Num Examples: 174\n",
      "Num Correct: 100\n",
      "Pct Correct: 57.47\n",
      "\n",
      "Not Math\n",
      "Num Examples: 826\n",
      "Num Correct: 497\n",
      "Pct Correct: 60.17\n"
     ]
    }
   ],
   "source": [
    "print('Math')\n",
    "math_results = [res for res in validation_results_v2 if math_questions[res[0]]]\n",
    "calculate_pct_correct(validation_examples, math_results)\n",
    "\n",
    "print('\\nNot Math')\n",
    "non_math_coding_results = [res for res in validation_results_v2 if not math_questions[res[0]]]\n",
    "calculate_pct_correct(validation_examples, non_math_coding_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8be4e644-ac68-4814-85cb-d59054314404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optiona: undeploy the model (shouldn't cost anything extra to leave deployed).\n",
    "# !firectl undeploy {model_v2_id}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
