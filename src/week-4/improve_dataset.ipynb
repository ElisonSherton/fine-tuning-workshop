{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d568c49-3f18-43ed-83c3-08ab36c2e5f0",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To complete the following guide you will need to install the following packages:\n",
    "- fireworks-ai\n",
    "- numpy\n",
    "- pandas\n",
    "- pronouncing\n",
    "- requests\n",
    "- sentence-transformers\n",
    "- transformers\n",
    "\n",
    "You will also need:\n",
    "\n",
    "- Fireworks account (https://fireworks.ai/)\n",
    "- Fireworks API key\n",
    "- The firectl command-line interface (https://docs.fireworks.ai/tools-sdks/firectl/firectl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acdd56c2-87a7-419b-8c83-d65ee7e752dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from fireworks.client import Fireworks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pronouncing\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "embeddings_model = SentenceTransformer('Alibaba-NLP/gte-base-en-v1.5', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c8be34-8480-4aba-a6a2-6d58a374e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sign-in to your Fireworks account\n",
    "!firectl signin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a85cbf23-f79c-4f87-87e0-aff1ba92607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the FIREWORKS_API_KEY environment variable set to your account's key!\n",
    "# os.environ['FIREWORKS_API_KEY'] = 'XXX'\n",
    "\n",
    "client = Fireworks()\n",
    "\n",
    "# Replace the line below with your Fireworks account id\n",
    "account_id = 'XXX'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2450281-21b6-47aa-8cea-20acfa6401bf",
   "metadata": {},
   "source": [
    "## Problem Definition: Poem Generation\n",
    "\n",
    "*Note: The poem topics used in this example were synthetically generated by Claude 3 Opus*\n",
    "\n",
    "LLMs are capable of performing creative writing tasks. However, assessing the quality of such tasks, like poetry generation, is highly subjective.\n",
    "\n",
    "### Task\n",
    "In last week's notebook, we created a framework to quantitatively evaluate LLM-generated poetry. This week, you'll observe how to further improve this solution.\n",
    "\n",
    "As I am not a professional poet, I am unable to write high-level poetry myself. The ideal approach would be to search the web for high-quality poems that matches the style I want the LLM to generate. However, this method is very time-consuming. A more efficient approach is to use the \"critique and revise\" method, where the LLM first generates critiques on how each poem can be improved. We then ask the LLM to rewrite the poems based on these critiques. Finally, we fine-tune the LLM on the revised poems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d95223-0a67-4360-aa31-de6519545ee9",
   "metadata": {},
   "source": [
    "### Data\n",
    "The data can be found in the week-4 data folder.\n",
    "\n",
    "We will use the following datasets:\n",
    "- `./data/training_poem_topics.csv`\n",
    "- `./data/test_poem_topics.csv`\n",
    "\n",
    "Each of those datasets consists of 100 unique poem topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca29b5b2-f1fa-499d-95a2-c2042823c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('data/training_poem_topics.csv')\n",
    "test_data = pd.read_csv('data/test_poem_topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8ab54-43cf-4b4e-9964-63f4b40f0413",
   "metadata": {},
   "source": [
    "### Foundation Model Baseline\n",
    "Our first step is to generate a poem for each of the topics in the training data using a foundation_model. We will then use the critique and revise method to improve upon these poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ee8f3b7-69ad-46aa-8e5d-1392008d7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a csv file with a list of topics, generates a poem for each topic\n",
    "system_message = 'You are a professional poet. Write a unique and original contemporary poem about the topic suggested by the user. Your response should contain ONLY the content of the poem.'\n",
    "def generate_poems(model, df):\n",
    "    responses = list()\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "              {\"role\": \"system\", \"content\": system_message},\n",
    "              {\"role\": \"user\", \"content\": row[1]['topic']}\n",
    "            ],\n",
    "        )\n",
    "        response = response.choices[0].message.content\n",
    "        responses.append(response)    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20470295-d4b7-4086-bee5-c5f190cfe855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first generate poems for poetry topics in our training set\n",
    "llama_70b_training_poems = generate_poems('accounts/fireworks/models/llama-v3-70b-instruct', training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cf2abe-9ae7-4270-8f8f-ad114251a409",
   "metadata": {},
   "source": [
    "### Critique\n",
    "In the critique step, we create a scoring rubric and ask an LLM to generate improvements to the previously created poems based on the rubric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cabe42c-9ade-430e-a819-c726884e3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now use our scoring rubric to generate a list of critiques about each poem\n",
    "poem_guidelines = \"\"\"- Is the poem original?\n",
    "- Does the poem contain beauty, power, education or entertainment?\n",
    "- is the message of the poem clear? Is it a good message, or is it of little value to anyone?\n",
    "- Is the poem clear in its expression? Does it maintain coherence throughout?\n",
    "- If the poem is written in rhyming verse, then it should be rated according to how well the rhymes fit, not only with each other, but with the flow and the intended nuance of meaning the verse demands.\n",
    "- What form does the poem take? Is it a sonnet, free verse, haiku, etc.? How does the form contribute to the poem's impact?\n",
    "- Does the poet us the best possible choice of words in the poem? A person can ball, cry, sob, whimper, and shed tears, but which term would best fit the mood the poet is trying to convey?\"\"\"\n",
    "\n",
    "poem_crtique_rubric = f'''You are professional poet responsible for assessing the quality of AI generated poems.\n",
    "\n",
    "Assessment Guidelines:\n",
    "{poem_guidelines}\n",
    "\n",
    "Given the above guidelines, provide a list of ways that the poem could be improved.'''\n",
    "\n",
    "def critique_poems(poems, evaluation_model):\n",
    "    critiques = list()\n",
    "    for poem in poems:\n",
    "        response = client.chat.completions.create(\n",
    "            model=evaluation_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": poem_crtique_rubric},\n",
    "                {\"role\": \"user\", \"content\": poem}\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        try: \n",
    "            response = response.choices[0].message.content\n",
    "            critiques.append(response)\n",
    "        except json.JSONDecodeError as jde:\n",
    "            continue\n",
    "\n",
    "    return critiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9704887f-721c-4024-b256-8e0ef324a6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_70b_training_critiques = critique_poems(llama_70b_training_poems, 'accounts/fireworks/models/llama-v3-70b-instruct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9eff6b-4702-4968-83ba-95581c0721af",
   "metadata": {},
   "source": [
    "### Revise\n",
    "In the revise step, we create a new prompt that tells the LLM to generate a revised poem, given the previously generated critiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9faf4fa-3784-4966-bf96-02d970fb223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now give the LLM both the poem and the critiques, and tell it to improve the poem based on the following critiques.\n",
    "improvement_sys_message = '''You are a professional poet. Improve the poem, given the following critiques.\n",
    "\n",
    "Your response must ONLY contain the content of the improved poem. DO NOT TELL ME YOUR CHANGES, JUST GIVE ME THE REVISED POEM!'''\n",
    "\n",
    "def generate_improved_poems(model, poems, critiques):\n",
    "    responses = list()\n",
    "    for i, poem in enumerate(poems):\n",
    "\n",
    "        user_message = f''''\n",
    "poem:      \n",
    "{poem}\n",
    "\n",
    "critiques:\n",
    "{critiques[i]}'''\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "              {\"role\": \"system\", \"content\": improvement_sys_message},\n",
    "              {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "        )\n",
    "        response = response.choices[0].message.content\n",
    "        responses.append(response)    \n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e109d8f-6764-465e-802d-e2f224fd4dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_70b_training_improved_poems = generate_improved_poems('accounts/fireworks/models/llama-v3-70b-instruct', llama_70b_training_poems, llama_70b_training_critiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6616c-6b12-4076-9085-a81066d5f3ee",
   "metadata": {},
   "source": [
    "### Fine-Tuning\n",
    "We know fine-tune a smaller LLM using the revised poems. This is similar to the knowledge distillation method from last week's notebook, except we are fine-tuning on the revised poems of the larger model, rather than the original poems that it generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e9cb57c-8342-43dd-be96-8c0cfe233e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the improved poems to fireworks as our fine-tuning dataset\n",
    "def format_poem_for_fireworks(topic, poem):\n",
    "    return {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_message}, \n",
    "        {\"role\": \"user\", \"content\": topic}, \n",
    "        {\"role\": \"assistant\", \"content\": poem}\n",
    "    ]}\n",
    "\n",
    "topics = training_data['topic'].tolist()\n",
    "json_objs = list()\n",
    "for i, poem in enumerate(llama_70b_training_improved_poems):\n",
    "    msg = {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_message}, \n",
    "        {\"role\": \"user\", \"content\": topics[i]}, \n",
    "        {\"role\": \"assistant\", \"content\": poem}\n",
    "    ]}    \n",
    "    json_objs.append(msg)\n",
    "\n",
    "dataset_file_name = 'poem_training_data.jsonl'\n",
    "dataset_id = 'improved-poem-data-v1'\n",
    "with open(dataset_file_name, 'w') as f:\n",
    "    for obj in json_objs:\n",
    "        json.dump(obj, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "532ba7c4-d8a3-44fd-99da-013410862a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload our dataset to fireworks\n",
    "!firectl create dataset {dataset_id} {dataset_file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "759f5765-bbc1-419d-b822-ebed16fb28da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fine-tuning job\n",
    "!firectl create fine-tuning-job --settings-file poem_generation_fine_tuning_config.yaml --display-name improved-poems-v1 --dataset {dataset_id} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f544dafc-10a8-4f2a-92af-9e0f13e20b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE THAT THIS ID WILL CHANGE WHEN YOU RUN THE FINE-TUNING JOB ON YOUR ACCOUNT!!!\n",
    "# The model id is printed in the stdout of the cell above as Name: accounts/{account_id}/fineTuningJobs/{ft_model_id}\n",
    "ft_model_id = '3dd6bdfb938546d88a7db95673124266' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39db4642-5075-4e80-915d-d9b0d96ba297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the State of the fine-tuning job is listed as COMPLETED (~10-20 minutes)\n",
    "!firectl get fine-tuning-job {ft_model_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33bf50b-0a15-4361-acb8-38a427dfd1fc",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Finally, we evaluate the fine-tuned model on our test data. In the previous weeks notebook, the knowledge distillation method resulted in an average LLM judge score of 8.21. We expect to receive a higher score now that we are fine-tuning on the revised poems rather than the initial poems that the large model generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08b52e9f-8948-42d0-9d07-7d5d8515311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the fine-tuned model\n",
    "!firectl deploy {ft_model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22d352a5-5693-4b1f-b43f-b08321c1de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the the Deploymed Model Refs lists the state of the model as \"DEPLOYED\" (~5-20 minutes).\n",
    "!firectl get model {ft_model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8cdea9fb-78cc-4293-ac5a-6f5597327d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate poems on the test set using our fine-tuned model\n",
    "ft_poems = generate_poems(f'accounts/{account_id}/models/{ft_model_id}', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0297ba4-6533-4763-a12f-2579a9665200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate poems based on their average length (# of characters)\n",
    "def calculate_avg_length(poems):\n",
    "    return int(np.mean([len(poem) for poem in poems]))\n",
    "\n",
    "# Evaluate poems based on the pct of stanzas that contain a rhyme\n",
    "def calculate_rhyming_fct(poem):\n",
    "    stanzas = poem.split('\\n\\n')\n",
    "    stanzas = [stanza for stanza in stanzas if len(stanza.split('\\n')) >= 1]\n",
    "    \n",
    "    num_rhyming_stanzas = 0\n",
    "    for stanza in stanzas:\n",
    "        lines = stanza.split('\\n')\n",
    "        end_words = [line.split(' ')[-1].strip('.?!\"\\',') for line in lines]\n",
    "        found_rhyme = False\n",
    "        for i in range(len(end_words)):\n",
    "            for j in range(i + 1, len(end_words)):\n",
    "                found_rhyme = True if found_rhyme or (end_words[j] in pronouncing.rhymes(end_words[i])) else False\n",
    "                \n",
    "        if found_rhyme:\n",
    "            num_rhyming_stanzas += 1\n",
    "\n",
    "    if not len(stanzas):\n",
    "        print(poem)\n",
    "    return num_rhyming_stanzas / len(stanzas)\n",
    "\n",
    "# Evaluate poems based on how often they have a positive sentiment\n",
    "def has_positive_sentiment(poem):\n",
    "    try:\n",
    "        sentiment = sentiment_pipeline(poem)[0]\n",
    "        return True if sentiment['label'] == 'POSITIVE' else False\n",
    "    except:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9cbb3fd-1bf0-4321-b511-fbc9e6bbf0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic Evaluation\n",
      "Average Length: 1255\n",
      "Rhyming Pct: 73%\n",
      "Positive Sentiment: 92%\n"
     ]
    }
   ],
   "source": [
    "# Calculate heuristics of our fine-tuned poems\n",
    "print(\"Heuristic Evaluation\")\n",
    "print(f'Average Length: {calculate_avg_length(ft_poems)}')\n",
    "print(f\"Rhyming Pct: {int(100 * np.mean([calculate_rhyming_fct(poem) for poem in ft_poems]))}%\")\n",
    "print(f\"Positive Sentiment: {int(100 * np.mean([has_positive_sentiment(poem) for poem in ft_poems]))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87c40a26-72af-436b-a186-659879895d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate poems using the LLM as a Judge strategy\n",
    "poem_evaluation_rubric = f'''You are professional poet responsible for assessing the quality of AI generated poems.\n",
    "\n",
    "Score each poem on a scale of 0 to 10, where 10 represents the best possible poem.\n",
    "\n",
    "Scoring Guidelines:\n",
    "{poem_guidelines}\n",
    "\n",
    "Think through your reasoning step-by-step and explain your reasoning. Steps for judging a poem:\n",
    "1. Read the Poem Multiple Times: Read it aloud and silently to capture both the meaning and the sound.\n",
    "2. Take Notes: Jot down initial impressions, notable phrases, and any questions that arise.\n",
    "3. Analyze the Elements: Break down the poem into its components (content, structure, language, sound).\n",
    "4. Reflect on Your Experience: Consider your emotional response and personal connection to the poem.\n",
    "\n",
    "The last line in your response MUST be a json object {{\"score\": XXX}}, where XXX is the score you are giving the response.'''\n",
    "\n",
    "def evaluate_poems(poems, evaluation_model):\n",
    "    scores = list()\n",
    "    for poem in poems:\n",
    "        response = client.chat.completions.create(\n",
    "            model=evaluation_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": poem_evaluation_rubric},\n",
    "                {\"role\": \"user\", \"content\": poem}\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        try: \n",
    "            response = response.choices[0].message.content\n",
    "            score = int(json.loads(response.split('\\n')[-1])['score'])  \n",
    "            scores.append(score)\n",
    "        except json.JSONDecodeError as jde:\n",
    "            continue\n",
    "        \n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd47aa0d-6d32-4d51-83e8-ba7a77b39694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg LLM Judge Score: 8.32\n"
     ]
    }
   ],
   "source": [
    "# Use the LLM to evaluate our fine-tuned model\n",
    "ft_avg_score = evaluate_poems(ft_poems, 'accounts/fireworks/models/llama-v3-70b-instruct')\n",
    "print(f\"Avg LLM Judge Score: {round(ft_avg_score , 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c4f7d0b-050c-4812-b07a-cbcadd6007c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undeploy the fine-tuned model (does not cost anything extra, but Fireworks may limit your number of deployed models).\n",
    "!firectl undeploy {ft_model_id}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
