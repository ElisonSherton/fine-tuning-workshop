{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b5b0b41-5df0-45bb-b825-e440c0cc3f76",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To complete the following guide you will need to install the following packages:\n",
    "- fireworks-ai\n",
    "- pandas\n",
    "\n",
    "You will also need:\n",
    "\n",
    "- Fireworks account\n",
    "- Fireworks API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f286740-3c83-4eb9-97e9-e62e2a1543dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q fireworks-ai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c70ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from fireworks.client import Fireworks\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c65cb8-6dce-401b-b900-a7d3b298a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the FIREWORKS_API_KEY environment variable set to your account's key!\n",
    "credentials = json.load(open(\"../credentials.json\", \"r\"))\n",
    "os.environ['FIREWORKS_API_KEY'] = credentials['fireworks_api_key']\n",
    "\n",
    "client = Fireworks(api_key = credentials['fireworks_api_key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c48b9d3-2265-4705-a5fc-dca12dccc602",
   "metadata": {},
   "source": [
    "## Problem Definition: Insurance Support Ticket Classifier\n",
    "\n",
    "*Note: The problem definition, data, and labels used in this example were synthetically generated by Claude 3 Opus*\n",
    "\n",
    "In the insurance industry, customer support plays a crucial role in ensuring client satisfaction and retention. Insurance companies receive a high volume of support tickets daily, covering a wide range of topics such as billing, policy administration, claims assistance, and more. Manually categorizing these tickets can be time-consuming and inefficient, leading to longer response times and potentially impacting customer experience.\n",
    "\n",
    "### Task\n",
    "In this exercise, we will evaluate the accuracy of various prompts on the test.tsv dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771ec57-1ec9-47e7-8ca5-7e90943f1c59",
   "metadata": {},
   "source": [
    "#### Labeled Data\n",
    "\n",
    "The data can be found in the week-1 `data` folder.\n",
    "\n",
    "We will use the following datasets:\n",
    "- `./data/train.tsv`\n",
    "- `./data/test.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a48d42d-b288-4a5d-929f-470f38213593",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "test_examples = pd.read_csv('data/test.tsv', sep='\\t')\n",
    "\n",
    "# In order to not leak information about the test labels into our prompts, the list of possible categories will be defined \n",
    "# based on the training labels. We'll discuss train/test splits more during week 2.\n",
    "categories = sorted(training_examples['label'].unique().tolist())\n",
    "categories_str = '\\n'.join(categories)\n",
    "\n",
    "test_tickets = test_examples['text'].tolist()\n",
    "test_labels = test_examples['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d21180e-8dcb-472d-8bb5-028a20f0a95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm considering moving my insurance policies to your firm. Is it possible for you to provide a quote that offers the same or better coverage compared to what I currently have?\n",
      "Quotes and Proposals\n"
     ]
    }
   ],
   "source": [
    "idx = random.choice(range(len(training_examples[\"text\"])))\n",
    "print(training_examples[\"text\"][idx], training_examples[\"label\"][idx], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bdf3ed6-54ac-4c20-b1f9-6d5c64a9e80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Billing Disputes', 9),\n",
       " ('General Inquiries', 9),\n",
       " ('Coverage Explanations', 8),\n",
       " ('Claims Disputes', 8),\n",
       " ('Billing Inquiries', 6),\n",
       " ('Policy Administration', 6),\n",
       " ('Claims Assistance', 6),\n",
       " ('Account Management', 6),\n",
       " ('Quotes and Proposals', 5),\n",
       " ('Policy Comparisons', 5)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(training_examples[\"label\"]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c596b097-aef6-4674-9b67-eef7e47bce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses an LLM to predicted class labels for a list of support tickets\n",
    "def classify_tickets(tickets, prompt_generator, model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\"):\n",
    "    responses = list()\n",
    "\n",
    "    for ticket in tickets:\n",
    "        user_prompt = prompt_generator(ticket)\n",
    "    \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                { \"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            # setting temperature to 0 for this use case, so that responses are as deterministic as possible\n",
    "            temperature=0, \n",
    "            stop=[\"</category>\"],\n",
    "            max_tokens=2048,\n",
    "        )\n",
    "        response = response.choices[0].message.content.split(\"<category>\")[1].strip()\n",
    "        responses.append(response)\n",
    "\n",
    "    return responses\n",
    "\n",
    "\n",
    "# Calculates the percent of predictions we classified correctly\n",
    "def evaluate_accuracy(predicted, actual):\n",
    "    num_correct = sum([predicted[i] == actual[i] for i in range(len(actual))])\n",
    "    return round(100 * num_correct / len(actual), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6dd0d-fe00-4263-b964-c05fe86105fe",
   "metadata": {},
   "source": [
    "### Classification with a simple prompt\n",
    "\n",
    "We will first evaluate the accuracy of the LLM on a simple prompt that does not used any advanced prompt engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a7c4842-9446-438e-ad1f-654a44ae0e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_prompt(ticket):\n",
    "    return f\"\"\"classify a customer support ticket into one of the following categories:\n",
    "<categories>\n",
    "{categories_str}\n",
    "</categories>\n",
    "\n",
    "Here is the customer support ticket:    \n",
    "<ticket>{ticket}</ticket>\n",
    "\n",
    "Respond using this format:\n",
    "<category>The category label you chose goes here</category>\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88b3a214-bd8d-45a5-a3cb-fc3390c86223",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_responses = classify_tickets(\n",
    "    tickets=test_tickets, \n",
    "    prompt_generator=create_simple_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd38d128-5ef8-4b2f-ba92-47f069066aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"simple_responses_results.json\", \"w\") as f:\n",
    "    json.dump({\"responses\": simple_responses, \"labels\": test_labels}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3094aa9-80d6-4ef3-a708-4feab1679b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Prompt Accuracy: 54.41%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_accuracy(simple_responses, test_labels)\n",
    "print(f\"Initial Prompt Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc5aeec-daff-4e29-9c55-2b8437ec1563",
   "metadata": {},
   "source": [
    "### Classification with an improved prompt\n",
    "\n",
    "This prompt builds upon the simple prompt, and improves the accuracy of the classification by applying the following techniques:\n",
    "- chain-of-thought: makes the LLM reflect on its reasoning before providing a response\n",
    "- few-shot learning: provides examples within the context of the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c4d5e9-ddb7-455d-9570-e911b270afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve one example from each of the k most popular labels\n",
    "def retrieve_few_shot_examples(df, k=5):\n",
    "    # Count the frequency of each label\n",
    "    label_counts = df['label'].value_counts()\n",
    "\n",
    "    # Identify the k most common labels\n",
    "    top_labels = label_counts.head(k).index\n",
    "\n",
    "    # Retrieve a single row for each of these labels\n",
    "    rows = df[df['label'].isin(top_labels)].groupby('label').first().reset_index()\n",
    "\n",
    "    # Convert each row to the example string format required by the prompt\n",
    "    example_strs = list()\n",
    "    for idx, row in rows.iterrows():\n",
    "        example_strs.append(f\"<example><ticket>{row['text']}</ticket><label>{row['label']}</label></example>\")\n",
    "\n",
    "    return '\\n'.join(example_strs)\n",
    "\n",
    "few_shot_examples = retrieve_few_shot_examples(training_examples)\n",
    "\n",
    "def create_improved_prompt(ticket):\n",
    "    return f\"\"\"classify a customer support ticket into one of the following categories:\n",
    "<categories>\n",
    "{categories_str}\n",
    "</categories>\n",
    "\n",
    "Here is the customer support ticket:    \n",
    "<ticket>{ticket}</ticket>\n",
    "\n",
    "Use the following examples to help you classify the query:\n",
    "<examples>\n",
    "{few_shot_examples}\n",
    "</examples>\n",
    "\n",
    "First you will think step-by-step about the problem in the scratchpad tag.\n",
    "You should consider all the information provided and create a concrete argument for your classification.\n",
    "\n",
    "Respond using this format:\n",
    "<response>\n",
    "  <scratchpad>Your thoughts and analysis go here</scratchpad>\n",
    "  <category>The category label you chose goes here</category>\n",
    "</response>\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb438a76-33fd-4b0f-9715-8dc526ad3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_responses = classify_tickets(\n",
    "    tickets=test_tickets, \n",
    "    prompt_generator=create_improved_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65177c58-6d9b-4430-96dc-d8057e020fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Prompt Accuracy: 70.59%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_accuracy(improved_responses, test_labels)\n",
    "print(f\"Improved Prompt Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcfd23b2-0a1d-4f1a-94f2-84c3d635ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"improved_responses_results.json\", \"w\") as f:\n",
    "    json.dump({\"responses\": improved_responses, \"labels\": test_labels}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1455c977-6d77-4c39-90e9-3b30b8fc7e90",
   "metadata": {},
   "source": [
    "# Send one example per label instead of only sending 1 of the top class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32047619-8bf1-44b7-a671-15b262856b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve one example from each of the labels\n",
    "def retrieve_few_shot_examples(df):\n",
    "    # Retrieve a single row for each of these labels\n",
    "    # Randomize the selection instead of picking the first one from each group\n",
    "    rows = df.groupby('label').sample(1, random_state = 42).reset_index()\n",
    "\n",
    "    # Convert each row to the example string format required by the prompt\n",
    "    example_strs = list()\n",
    "    for idx, row in rows.iterrows():\n",
    "        example_strs.append(f\"<example><ticket>{row['text']}</ticket><label>{row['label']}</label></example>\")\n",
    "    return '\\n'.join(example_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce7f6b63-a355-4420-899c-92b4896949ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = retrieve_few_shot_examples(training_examples)\n",
    "\n",
    "def create_improved_prompt(ticket):\n",
    "    return f\"\"\"classify a customer support ticket into one of the following categories:\n",
    "<categories>\n",
    "{categories_str}\n",
    "</categories>\n",
    "\n",
    "Here is the customer support ticket:    \n",
    "<ticket>{ticket}</ticket>\n",
    "\n",
    "Use the following examples to help you classify the query:\n",
    "<examples>\n",
    "{few_shot_examples}\n",
    "</examples>\n",
    "\n",
    "First you will think step-by-step about the problem in the scratchpad tag.\n",
    "You should consider all the information provided and create a concrete argument for your classification.\n",
    "\n",
    "Respond using this format:\n",
    "<response>\n",
    "  <scratchpad>Your thoughts and analysis go here</scratchpad>\n",
    "  <category>The category label you chose goes here</category>\n",
    "</response>\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "522c53da-19e3-49b4-8c53-02aa3afc3c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_test_responses = classify_tickets(\n",
    "    tickets=test_tickets, \n",
    "    prompt_generator=create_improved_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "800afcff-56ff-45d6-b109-489a90424302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Prompt Accuracy: 75.0%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_accuracy(second_test_responses, test_labels)\n",
    "print(f\"Improved Prompt Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7692dc01-fb9a-4171-a989-9db61b793b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"ground_truth\": test_labels, \"simple_responses\": simple_responses, \n",
    "                   \"improved_responses\": improved_responses, \"one_example_per_class_responses\": second_test_responses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fb53ba6-41a7-4c31-899d-f25a9a8561b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of simple_responses\n",
      "\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "   Account Management       1.00      0.33      0.50         6\n",
      "     Billing Disputes       1.00      0.11      0.20         9\n",
      "    Billing Inquiries       0.38      0.83      0.53         6\n",
      "    Claims Assistance       0.50      0.50      0.50         6\n",
      "      Claims Disputes       1.00      0.67      0.80         9\n",
      "Coverage Explanations       0.45      1.00      0.62         9\n",
      "    General Inquiries       1.00      0.00      0.00         7\n",
      "Policy Administration       0.50      0.83      0.62         6\n",
      "   Policy Comparisons       0.56      1.00      0.71         5\n",
      " Quotes and Proposals       1.00      0.20      0.33         5\n",
      "\n",
      "             accuracy                           0.54        68\n",
      "            macro avg       0.74      0.55      0.48        68\n",
      "         weighted avg       0.75      0.54      0.48        68\n",
      "\n",
      "Performance of improved_responses\n",
      "\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "   Account Management       1.00      0.67      0.80         6\n",
      "     Billing Disputes       0.88      0.78      0.82         9\n",
      "    Billing Inquiries       0.67      0.67      0.67         6\n",
      "    Claims Assistance       0.71      0.83      0.77         6\n",
      "      Claims Disputes       1.00      0.78      0.88         9\n",
      "Coverage Explanations       0.62      0.89      0.73         9\n",
      "    General Inquiries       0.60      0.43      0.50         7\n",
      "Policy Administration       0.44      0.67      0.53         6\n",
      "   Policy Comparisons       0.62      1.00      0.77         5\n",
      " Quotes and Proposals       1.00      0.20      0.33         5\n",
      "\n",
      "             accuracy                           0.71        68\n",
      "            macro avg       0.75      0.69      0.68        68\n",
      "         weighted avg       0.76      0.71      0.70        68\n",
      "\n",
      "Performance of one_example_per_class_responses\n",
      "\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "   Account Management       0.71      0.83      0.77         6\n",
      "     Billing Disputes       0.71      0.56      0.62         9\n",
      "    Billing Inquiries       0.50      0.50      0.50         6\n",
      "    Claims Assistance       1.00      1.00      1.00         6\n",
      "      Claims Disputes       1.00      1.00      1.00         9\n",
      "Coverage Explanations       0.62      0.89      0.73         9\n",
      "    General Inquiries       1.00      0.29      0.44         7\n",
      "Policy Administration       0.60      1.00      0.75         6\n",
      "   Policy Comparisons       0.83      1.00      0.91         5\n",
      " Quotes and Proposals       1.00      0.40      0.57         5\n",
      "\n",
      "             accuracy                           0.75        68\n",
      "            macro avg       0.80      0.75      0.73        68\n",
      "         weighted avg       0.79      0.75      0.73        68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    print(f\"Performance of {df.columns[i]}\\n\")\n",
    "    trues = df.iloc[:, 0]\n",
    "    preds = df.iloc[:, i]\n",
    "    print(classification_report(trues, preds, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f09b84c-c602-419c-80ad-1ccb48666031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Policy Administration    6\n",
       "Claims Assistance        6\n",
       "Account Management       6\n",
       "Quotes and Proposals     5\n",
       "Policy Comparisons       5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the bottom 5 labels based on frequenct\n",
    "training_examples.label.value_counts().tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd36a8e-cb75-417a-9ca9-2f16afd0b4d9",
   "metadata": {},
   "source": [
    "Seems like adding one example per class is giving better performance on bottom 5 classes without harming the top 5 classes performance too much..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
