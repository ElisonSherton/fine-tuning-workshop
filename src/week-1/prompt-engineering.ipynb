{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c70ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from datasets import load_dataset\n",
    "from fireworks.client import Fireworks\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c65cb8-6dce-401b-b900-a7d3b298a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the FIREWORKS_API_KEY set to your account's key!\n",
    "client = Fireworks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23efc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a datset containing human preferences from the chatbot arena. When a human types a message, they are sent responses from two\n",
    "# different chatbots. The human then votes on which response they prefer. Throughout this course, I am going to fine-tune a model to predict\n",
    "# human chatbot preferences. For this week's assignment, I will perform prompt engineering to get a baseline of how well llama3-8b-instruct\n",
    "# performs at this task before performing fine-tuning\n",
    "# For more details on the dataset: https://huggingface.co/datasets/lmsys/chatbot_arena_conversations\n",
    "dataset = load_dataset(\"lmsys/chatbot_arena_conversations\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a48d42d-b288-4a5d-929f-470f38213593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, I am only going to look at single-turn chats, where the user declared a winner after a single response from the bot.\n",
    "examples = [example for example in dataset if example['turn'] == 1]\n",
    "\n",
    "# The query the user sent to both bots should be exactly the same, so that we are fairly judging the responses. This should be always be\n",
    "# the case for this dataset. This line just acts as a sanity check.\n",
    "examples = [example for example in examples if example['conversation_a'][0]['content'] == example['conversation_b'][0]['content']]\n",
    "\n",
    "# I am using the first 1000 responses to get an estimate of how well the baseline model performs. Using the entire dataset will take longer\n",
    "# and cost more, and is not needed.\n",
    "# Note that you DO NOT need 1,000 examples for your dataset for this assignment (but you can if you want). Just use at least 10 examples \n",
    "# so that you can get an idea of how the base model performs.\n",
    "training_examples = examples[:1000]\n",
    "\n",
    "# To improve the prompt, I am going to use few-shot learning. We want the examples used in few-shot learning to be different than the\n",
    "# examples we used to evaluate the model quality\n",
    "few_shot_learning_examples = examples[1000:1003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8deb743-1bfe-454d-97b5-d2c8ee1393da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My initial prompt is very simple, and does not include any prompt engineering techniques\n",
    "initial_sys_prompt = f'''Choose the better chatbot response between model_a and model_b.\n",
    "\n",
    "Your response MUST be ONLY the JSON object {{\"winner\": XXX}}. XXX can only equal \"model_a\", \"model_b\", or \"tie\".'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "101b033f-486d-428a-a136-1972dcdc2c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating 0 examples\n",
      "Finished generating 100 examples\n",
      "Finished generating 200 examples\n",
      "Finished generating 300 examples\n",
      "Finished generating 400 examples\n",
      "Finished generating 500 examples\n",
      "Finished generating 600 examples\n",
      "Finished generating 700 examples\n",
      "Finished generating 800 examples\n",
      "Failed to parse JSON for example 801.\n",
      "Failed to parse JSON for example 804.\n",
      "Finished generating 900 examples\n"
     ]
    }
   ],
   "source": [
    "winners = list()\n",
    "\n",
    "for i, example in enumerate(training_examples):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Finished generating {i} examples\")\n",
    "    \n",
    "    user_query = example['conversation_a'][0]['content']\n",
    "    model_a_response = example['conversation_a'][1]['content']\n",
    "    model_b_response = example['conversation_b'][1]['content']\n",
    "\n",
    "    user_prompt = f\"\"\"user query: {user_query}\n",
    "    \n",
    "model_a response: {model_a_response}\n",
    "\n",
    "model_b response: {model_b_response}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"accounts/fireworks/models/llama-v3-8b-instruct\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": initial_sys_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            }\n",
    "        ],\n",
    "        # setting temperature to 0 for this use case, so that responses are as deterministic as possible\n",
    "        temperature=0, \n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        winner = json.loads(content.split('\\n')[-1])[\"winner\"]\n",
    "        winners.append((i, winner))\n",
    "    except json.JSONDecodeError as jde:\n",
    "        print(f\"Failed to parse JSON for example {i}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3094aa9-80d6-4ef3-a708-4feab1679b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Prompt Accuracy: 58.41683366733467%\n"
     ]
    }
   ],
   "source": [
    "num_correct = sum([training_examples[winner[0]]['winner'] == winner[1] for winner in winners])\n",
    "print(f\"Initial Prompt Accuracy: {100 * num_correct / len(winners)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb438a76-33fd-4b0f-9715-8dc526ad3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I first perform prompt engineering improving clarity and specificity of the system prompt.\n",
    "improved_sys_prompt = f'''You are evaluating chatbot responses.\n",
    "\n",
    "In the conversation below, a human sent a query to two different chatbots. The human then voted for whether they preferred the response from model_a or model_b. Your task is to predict which response the human preferred.\n",
    "\n",
    "Choose the better chatbot response between model_a and model_b.\n",
    "\n",
    "Your response MUST be ONLY the JSON object {{\"winner\": XXX}}. XXX can only equal \"model_a\", \"model_b\", or \"tie\".'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65177c58-6d9b-4430-96dc-d8057e020fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will also use the \"few-shot learning\" prompt engineering method. This method involves including a few examples in the prompt, so that\n",
    "# the model can start to get an idea of how it should respond.\n",
    "def create_example_message(example):\n",
    "\n",
    "    example_user_msg = f\"\"\"user query: {example['conversation_a'][0]['content']}\n",
    "        \n",
    "model_a response: {example['conversation_a'][1]['content']}\n",
    "\n",
    "model_b response: {example['conversation_b'][1]['content']}\"\"\"\n",
    "    \n",
    "    example_asst_msg = json.dumps({\"winner\": example['winner']})\n",
    "\n",
    "    return (example_user_msg, example_asst_msg)\n",
    "\n",
    "example_0 = create_example_message(few_shot_learning_examples[0])\n",
    "example_1 = create_example_message(few_shot_learning_examples[1])\n",
    "example_2 = create_example_message(few_shot_learning_examples[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9912b391-fa8a-4886-b9b5-133ca43148e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating 0 examples\n",
      "Finished generating 100 examples\n",
      "Failed to parse JSON for example 129.\n",
      "Failed to parse JSON for example 137.\n",
      "Failed to parse JSON for example 198.\n",
      "Finished generating 200 examples\n",
      "Failed to parse JSON for example 260.\n",
      "Finished generating 300 examples\n",
      "Finished generating 400 examples\n",
      "Failed to parse JSON for example 465.\n",
      "Failed to parse JSON for example 467.\n",
      "Finished generating 500 examples\n",
      "Failed to parse JSON for example 504.\n",
      "Finished generating 600 examples\n",
      "Failed to parse JSON for example 648.\n",
      "Finished generating 700 examples\n",
      "Finished generating 800 examples\n",
      "Failed to parse JSON for example 844.\n",
      "Finished generating 900 examples\n",
      "Failed to parse JSON for example 922.\n",
      "Failed to parse JSON for example 956.\n"
     ]
    }
   ],
   "source": [
    "winners = list()\n",
    "\n",
    "for i, example in enumerate(training_examples):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Finished generating {i} examples\")\n",
    "    \n",
    "    user_query = example['conversation_a'][0]['content']\n",
    "    model_a_response = example['conversation_a'][1]['content']\n",
    "    model_b_response = example['conversation_b'][1]['content']\n",
    "\n",
    "    user_prompt = f\"\"\"user query: {user_query}\n",
    "    \n",
    "model_a response: {model_a_response}\n",
    "\n",
    "model_b response: {model_b_response}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"accounts/fireworks/models/llama-v3-8b-instruct\",\n",
    "        messages=[\n",
    "            # few-shot learning\n",
    "            {\"role\": \"system\", \"content\": improved_sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": example_0[0]},\n",
    "            {\"role\": \"assistant\", \"content\": example_0[1]},\n",
    "            {\"role\": \"user\", \"content\": example_1[0]},\n",
    "            {\"role\": \"assistant\", \"content\": example_1[1]},\n",
    "            {\"role\": \"user\", \"content\": example_2[0]},\n",
    "            {\"role\": \"assistant\", \"content\": example_2[1]},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        # setting temperature to 0 for this use case, so that responses are as deterministic as possible\n",
    "        temperature=0, \n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        winner = json.loads(content.split('\\n')[-1])[\"winner\"]\n",
    "        winners.append((i, winner))\n",
    "    except:\n",
    "        print(f\"Failed to parse JSON for example {i}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c5ddd0a-5ea8-4bcc-9599-e3af6817ebc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Prompt Accuracy: 52.881698685540954%\n"
     ]
    }
   ],
   "source": [
    "# I was not able to improve the accuracy through prompt engineering. This shows that prompt engineering is unsufficient for this task,\n",
    "# and I will need to perform fine-tuning to get better results. Next week, I will fine-tune my first model for this task to get\n",
    "# better results\n",
    "num_correct = sum([training_examples[winner[0]]['winner'] == winner[1] for winner in winners])\n",
    "print(f\"Improved Prompt Accuracy: {100 * num_correct / len(winners)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
