{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea04c4e0-b06c-44be-ba6e-9abc967c5999",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 18px;\">\n",
    "This notebook provides an example on how to fine-tune an LLM for item classification using the Fireworks API. In this example, we will classify items ordered by an oil facility.<br><br>\n",
    "<b>Steps:</b><br>\n",
    "0. Configure Environment<br>\n",
    "1. Curate Dataset<br>\n",
    "2. Train Model<br>\n",
    "3. Evaluate Results<br><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd9dfc-c388-408e-9376-0f2745aa8e7d",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 18px;\">\n",
    "**********************<br><b>STEP 0: Configure Environment</b><br><br>\n",
    "Install necessary libraries, import them, and configure Fireworks API access.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65c86e34-bcd2-422c-9af1-853eb0d674e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required Python libraries\n",
    "!pip install fireworks-ai pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4162827-52f5-45ac-8bfa-74bee9311360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE PROCEEDING, MAKE SURE YOU FOLLOW THE INSTRUCTIONS TO INSTALL FIRECTL, BASED ON YOUR ARCHITECTURE \n",
    "# https://readme.fireworks.ai/reference/installation-1\n",
    "\n",
    "# sign in to firectl\n",
    "!firectl signin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "381a5438-872d-4bfc-9761-43b7e41706f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import fireworks.client as fc\n",
    "from fireworks.client import Fireworks\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c734cde-d776-428a-9baa-664a53016466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to your Fireworks account id\n",
    "account_id = 'sdkramer10-5e98cb'\n",
    "\n",
    "# Uncomment the line below and set the value to your account's API key\n",
    "# fc.api_key = '<API KEY>'\n",
    "\n",
    "client = Fireworks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13a2132-056e-4e09-a38b-42a03cdac0fc",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 18px;\">\n",
    "**********************<br><b>STEP 1: Curate Dataset</b><br><br>\n",
    "Transform a CSV dataset containing item descriptions and classes into the format required for fine-tuning, and upload the dataset to Fireworks. For more details on the dataset requirements, refer to the <a href=\"https://docs.fireworks.ai/fine-tuning/fine-tuning-models#conversation\">Fireworks guide</a>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad66aa0a-1b58-473e-bb55-2ef0a829598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv dataset, which contains the item descriptions (under column \"Item\") and their correct classification (under column \"Class\").\n",
    "item_class_csv_url = 'item_classification.csv'\n",
    "df = pd.read_csv(item_class_csv_url)\n",
    "\n",
    "# Perform a train/test split of the data. Refer back to the \"Training Evaluation\" slides from the \"Train Your Model\" class \n",
    "# (https://gamma.app/docs/Train-Your-Model-fotbcos9eduoh0v) on why it's important to have a test set\n",
    "x_train, x_test, y_train, y_test = train_test_split( df['Item'], df['Class'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6e7d099-3008-468a-ae11-48123a2396c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the following item. Respond with ONLY the name of the appropriate class from the list below.\n",
      "\n",
      "CLASSES:\n",
      "Pressure Safety Device\n",
      "Piping\n",
      "Structure\n",
      "Pressure Vessel (VIE)\n",
      "FU Items\n",
      "Non Structural Tank\n",
      "Campaign\n",
      "Lifting \n",
      "Corrosion Monitoring\n",
      "Pressure Vessel (VII)\n",
      "Lifting\n",
      "Flare TIP\n",
      "Flame Arrestor\n",
      "Flare Tip\n",
      "Intelligent Pigging\n"
     ]
    }
   ],
   "source": [
    "# As discussed during the first workshop, the system message contains instructions for the LLM to perform a task. \n",
    "# For classification tasks, I recommend that you list the possible classes within the system message. \n",
    "# For base models, this ensures that the LLM responds with the correct class names.\n",
    "# For fine-tuned models, while including the class names isn't technically required, including them in the system message \n",
    "# helps the LLM learn more quickly.\n",
    "possible_classes = '\\n'.join(list(y_train.unique()))\n",
    "system_message = f'''Classify the following item. Respond with ONLY the name of the appropriate class from the list below.\n",
    "\n",
    "CLASSES:\n",
    "{possible_classes}'''\n",
    "\n",
    "print(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db19d372-e3b9-48fe-96df-70352652c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data into the format required by Fireworks for fine-tuning.\n",
    "json_objs = list()\n",
    "for i in range(len(x_train)):\n",
    "    msg = {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_message}, \n",
    "        {\"role\": \"user\", \"content\": x_train.iloc[i]}, \n",
    "        {\"role\": \"assistant\", \"content\": y_train.iloc[i]}\n",
    "    ]}  \n",
    "\n",
    "    json_objs.append(msg)\n",
    "\n",
    "dataset_file_name = 'item_classification.jsonl'\n",
    "dataset_name = 'item-class-v1'\n",
    "\n",
    "with open(dataset_file_name, 'w') as f:\n",
    "    for obj in json_objs:\n",
    "        json.dump(obj, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2014f264-06b3-47b3-abd5-b2b5bf594848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset to Fireworks\n",
    "!firectl create dataset {dataset_name} {dataset_file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f824f4-4fa7-41cc-aa48-52b90fa84e2d",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 18px;\">\n",
    "**********************<br><b>STEP 2: Train Model</b><br><br>\n",
    "Create a fine-tuning job within Fireworks. For more details on the firectl commands and how Fireworks implements fine-tuning, refer to the <a href=\"https://docs.fireworks.ai/fine-tuning/fine-tuning-models#starting-your-tuning-job\">FireWorks guide</a>.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84073b47-0dd9-4d1d-b38d-0f53f1b0ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fine-tuning job within Fireworks. For advanced users, you can optionally adjust the item_classification.yaml \n",
    "# file to tweak the training parameters. Refer back to the \"QLoRA Hyperparameters\" section of the \"Train Your Model\" workshop \n",
    "# (https://gamma.app/docs/Training-Your-Model-fotbcos9eduoh0v) for more details on what each parameter represents.\n",
    "!firectl create fine-tuning-job --settings-file item_classification.yaml  --display-name {dataset_name} --dataset {dataset_name} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6b708349-7899-4daf-83d4-25d85c21094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace model_id with the id of your fine-tuned model.\n",
    "# After executing the cell above, the first line in it's output will be \"Name: accounts/<ACCOUNT_ID>/fineTuningJobs/<MODEL_ID>\"\n",
    "model_id = 'c55a772445ed48f1bbad2b15c1508b86'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ada8693-e697-430f-b84a-2774f4b078bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the status of the fine-tuning job. Wait until the state says COMPLETED before continuing (~10-20 mins).\n",
    "!firectl get fine-tuning-job {model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0592063a-2b61-4c0f-ac1e-57884dc5aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model to a FireWorks endpoint\n",
    "!firectl deploy {model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f183be1b-357b-402a-a2b9-ccdd4bb3f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model's status. Wait until \"Deployed Model Refs\" says DEPLOYED before running reference (~15-20 mins).\n",
    "!firectl get model {model_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff82c53-b22d-4d37-91ff-e2a77b5d8e3b",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 18px;\">\n",
    "**********************<br><b>STEP 3: Evaluate Results</b><br><br>\n",
    "Evaluate the accuracy of the fine-tuned model and compare it to the accuracy of the base model.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dd4740a-b953-4a1d-8179-bac3e631220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(item_descriptions, model_name, system_message=system_message):\n",
    "    \"\"\"\n",
    "    Generate the predicted class for each item description using the FireWorks API.\n",
    "\n",
    "    This function iterates through a list of item descriptions, sends each description\n",
    "    to the FireWorks API, and collects the generated responses.\n",
    "\n",
    "    Args:\n",
    "        item_descriptions (list): A list of strings, where each string is an item description.\n",
    "        model_name (str): The name of the model to use for generating responses.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated responses, where each response corresponds to an item description.\n",
    "    \"\"\"    \n",
    "    responses = list()\n",
    "    \n",
    "    for item_descr in item_descriptions:      \n",
    "        msg = [\n",
    "              {\"role\": \"system\", \"content\": system_message},\n",
    "              {\"role\": \"user\", \"content\": item_descr}\n",
    "        ]\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=msg,\n",
    "            # Set the temp to 0 for tasks where there is a single correct answer, such as classification\n",
    "            temperature=0, \n",
    "        )\n",
    "    \n",
    "        response = response.choices[0].message.content\n",
    "        responses.append(response) \n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17914acf-91be-4e8c-96f3-62a99f5ad0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Test Set Accuracy: 50.0%\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on the test set using the base model (Llama 3 8B instruct), and calculate the accuracy.\n",
    "base_model_name = 'accounts/fireworks/models/llama-v3-8b-instruct'\n",
    "predictions = generate_responses(x_test.tolist(), base_model_name)\n",
    "\n",
    "num_correct = len([i for i in range(len(predictions)) if predictions[i] == y_test.iloc[i]])\n",
    "total = len(predictions)\n",
    "pct_accuracy = round(100 * num_correct / total, 2)\n",
    "print(f'Base Model Test Set Accuracy: {pct_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "797ee38b-0d48-493d-9032-318381acb55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Model Test Set Accuracy: 98.23%\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on the test set using the fine-tuned model (Llama 3 8B instruct), and calculate the accuracy.\n",
    "ft_model_name = f'accounts/{account_id}/models/{model_id}'\n",
    "predictions = generate_results(x_test.tolist(), ft_model_name)\n",
    "\n",
    "num_correct = len([i for i in range(len(predictions)) if predictions[i] == y_test.iloc[i]])\n",
    "total = len(predictions)\n",
    "pct_accuracy = round(100 * num_correct / total, 2)\n",
    "print(f'Fine-Tuned Model Test Set Accuracy: {pct_accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c8db80-6f25-4a32-8b9b-5dc2686ef32b",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 18px;\">\n",
    "**********************<br><b>Conclusion</b><br><br>\n",
    "After fine-tuning, the accuracy of item classification improved from 50% to 98.23%, demonstrating the significant impact of fine-tuning! Since this evaluation was performed on a test set not included in the training data, we can confidently expect these gains to apply to new items as well.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b327b09-317f-4b6b-a1d7-2cf36983b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: undeploy the model. Fireworks currently does not charge for deploy models, only for usage, \n",
    "# so up to you whether you want to keep it deployed\n",
    "!fireworks undeploy {model_id}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
