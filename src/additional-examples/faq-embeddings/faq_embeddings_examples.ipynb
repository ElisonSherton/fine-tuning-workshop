{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67716cd5-4059-4f14-b619-ad607b7c574c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To complete the following guide you will need to install the following packages:\n",
    "- transformers\n",
    "- sentence-transformers\n",
    "- accelerate\n",
    "\n",
    "You will also need access to a GPU with at least 24 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21edf9af-8f46-4a99-ab01-2b1c7ab4448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install \"transformers==4.41.2\" \"sentence-transformers==3.0.1\" \"accelerate==0.30.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22117dcd-3164-4924-95d9-39ef7a26eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import (\n",
    "    InformationRetrievalEvaluator\n",
    ")\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.util import cos_sim, semantic_search\n",
    "from sentence_transformers import SentenceTransformerTrainingArguments, SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import BatchSamplers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b137cca-1041-49a1-9b1d-5e7e91b36d2d",
   "metadata": {},
   "source": [
    "## Problem Definition: FAQ Embeddings\n",
    "\n",
    "A common use case for LLMs is enabling users to ask questions and retrieve answers from a document corpus. This typically involves generating embeddings for the question and each document, then calculating cosine similarity to identify the document most relevant to the question. However, the retrieved answers may not be ideal because the embeddings weren’t trained on the specifics of your company’s data. To improve results, the embeddings can be fine-tuned on your proprietary data.\n",
    "\n",
    "### Task\n",
    "In this example, we will fine-tune on open-source embeddings model using philschmid/finanical-rag-embedding-dataset, which includes 7,000 positive text pairs of questions and corresponding context from the 2023_10 NVIDIA SEC Filing. The results in this notebook show that the fine-tuned model results in an NDCG@10 that is ~7% higher than the base model.\n",
    "\n",
    "*Note: The code in this notebook is adapted from https://www.philschmid.de/fine-tune-embedding-model-for-rag*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0266f242-8d41-4ff3-b1c5-9766c61eb3cd",
   "metadata": {},
   "source": [
    "### Step 1: Dataset Curation\n",
    "\n",
    "We first retrieve our dataset from Hugging Face (https://huggingface.co/datasets/philschmid/finanical-rag-embedding-dataset) and perform a train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a323f46a-0741-4942-82f0-ef54d4186780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16e5465d6f944bcaef49b510993321c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8691c825e2bf42f4ac2e766977d44dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "238798"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"philschmid/finanical-rag-embedding-dataset\", split=\"train\")\n",
    " \n",
    "# rename columns\n",
    "dataset = dataset.rename_column(\"question\", \"anchor\")\n",
    "dataset = dataset.rename_column(\"context\", \"positive\")\n",
    " \n",
    "# Add an id column to the dataset\n",
    "dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    " \n",
    "# split dataset into a 10% test set\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    " \n",
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e360ea-9e5b-4dcd-b6b3-ce92983859bc",
   "metadata": {},
   "source": [
    "### Step 2: Evaluate Base Model\n",
    "\n",
    "After we created our dataset we want to create a baseline. A baseline provides use reference point against which the performance of your customized model can be measured. By evaluating the performance of a pretrained model on our specific dataset, we gain insights into the initial effectiveness and can identify areas for improvement.\n",
    "\n",
    "For our example, we will use the BAAI/bge-base-en-v1.5 model as our starting point. BAAI/bge-base-en-v1.5 is one of the strongest open embedding models for it size according to the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n",
    "\n",
    "For us the most important metric will be Normalized Discounted Cumulative Gain (NDCG) as it is a measure of ranking quality. It takes into account the position of the relevant document in the ranking and discounts it. The discounted value is logarithmic, which means that relevant documents are more important if they are ranked higher.\n",
    "\n",
    "For our evaluation corpus we will use all \"documents\" for potential retrieval from the train and test split and then query each question in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7091b89a-108a-4f86-ad46-74f4bc58693e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottkramer/.pyenv/versions/3.8.16/envs/fine-tuning-workshop/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acada547f737443f95248fb40b084fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74231b9211364ccaa678b1dcb68a375b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"BAAI/bge-base-en-v1.5\"  # Hugging Face model ID. This can be adapted depending on which embeddings model you wish to fine-tune\n",
    " \n",
    "# Load a model\n",
    "model = SentenceTransformer(model_id, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "# load test dataset\n",
    "test_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "train_dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n",
    "corpus_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    " \n",
    "# Convert the datasets to dictionaries\n",
    "corpus = dict(\n",
    "    zip(corpus_dataset[\"id\"], corpus_dataset[\"positive\"])\n",
    ")  # Our corpus (cid => document)\n",
    "queries = dict(\n",
    "    zip(test_dataset[\"id\"], test_dataset[\"anchor\"])\n",
    ")  # Our queries (qid => question)\n",
    " \n",
    "# Create a mapping of relevant document (1 in our case) for each query\n",
    "relevant_docs = {}  # Query ID to relevant documents (qid => set([relevant_cids])\n",
    "for q_id in queries:\n",
    "    relevant_docs[q_id] = [q_id]\n",
    " \n",
    "# Create an evaluator\n",
    "evaluator = InformationRetrievalEvaluator(\n",
    "    queries=queries,\n",
    "    corpus=corpus,\n",
    "    relevant_docs=relevant_docs,\n",
    "    score_functions={\"cosine\": cos_sim}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10eeeac6-ed5f-4b35-b18b-5e527aa4c0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_ndcg@10: 0.7624707202194887\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "results = evaluator(model)\n",
    " \n",
    "# # COMMENT IN for full results\n",
    "# print(results)\n",
    " \n",
    "# Print the main score\n",
    "key = f\"cosine_ndcg@10\"\n",
    "print(f\"{key}: {results[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa35b85-2e20-4ae3-b12a-699519f87909",
   "metadata": {},
   "source": [
    "### Step 3: Fine-Tune Model\n",
    "\n",
    "We are now ready to fine-tune our model. We will use[SentenceTransformersTrainer](https://sbert.net/docs/package_reference/sentence_transformer/trainer.html#sentencetransformertrainer). This is a subclass of the Trainer class from the transformers library. We will use the SentenceTransformerTrainingArguments class to specify all the training parameters.\n",
    "\n",
    "In this example, we use the MultipleNegativesRankingLoss function. This loss function requires only positive examples, and automatically creates negative examples out of the (query, document) pairs that do not exist in the dataset\n",
    "\n",
    "Note that this example performs full parameter fune-tuning, NOT PEFT/QLoRA. Embedding models are typically smaller than LLMs (the model used in this example has 109 MM params), which makes full parameter fine-tuning feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ecd0a1-cf7d-40f6-9715-5e584e475d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train dataset again\n",
    "train_dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53d42d60-ca81-482d-aa2b-c7aa0bb5fb59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T19:20:58.400680Z",
     "iopub.status.busy": "2024-11-14T19:20:58.400680Z",
     "iopub.status.idle": "2024-11-14T19:20:59.217841Z",
     "shell.execute_reply": "2024-11-14T19:20:59.216638Z",
     "shell.execute_reply.started": "2024-11-14T19:20:58.400680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define training loss function\n",
    "train_loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# define training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"bge-financial-fine-tuned\",      # output directory and hugging face model ID\n",
    "    num_train_epochs=4,                         # number of epochs\n",
    "    per_device_train_batch_size=32,             # train batch size\n",
    "    gradient_accumulation_steps=16,             # for a global batch size of 512\n",
    "    per_device_eval_batch_size=16,              # evaluation batch size\n",
    "    warmup_ratio=0.1,                           # warmup ratio\n",
    "    learning_rate=2e-5,                         # learning rate, 2e-5 is a good value\n",
    "    lr_scheduler_type=\"cosine\",                 # use constant learning rate scheduler\n",
    "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
    "    # tf32=True,                                  # use tf32 precision\n",
    "    # bf16=True,                                  # use bf16 precision\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    eval_strategy=\"epoch\",                      # evaluate after each epoch\n",
    "    save_strategy=\"epoch\",                      # save after each epoch\n",
    "    logging_steps=10,                           # log every 10 steps\n",
    "    save_total_limit=3,                         # save only the last 3 models\n",
    "    load_best_model_at_end=True,                # load the best model when training ends\n",
    "    metric_for_best_model=\"eval_cosine_ndcg@10\",  # Optimizing for the best ndcg@10 score\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model, # bg-base-en-v1\n",
    "    args=args,  # training arguments\n",
    "    train_dataset=train_dataset.select_columns(\n",
    "        [\"positive\", \"anchor\"]\n",
    "    ),  # training dataset\n",
    "    loss=train_loss,\n",
    "    evaluator=evaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ea69182-c164-418e-bd8c-5c4fc9d1ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b252c01e-c314-4107-801b-698eb82d046b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T19:13:08.410787Z",
     "iopub.status.busy": "2024-11-14T19:13:08.409358Z",
     "iopub.status.idle": "2024-11-14T19:13:10.151272Z",
     "shell.execute_reply": "2024-11-14T19:13:10.149235Z",
     "shell.execute_reply.started": "2024-11-14T19:13:08.410729Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the best model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae31c867-3ff5-4e4f-870b-1eea00134762",
   "metadata": {},
   "source": [
    "### Step 4: Use the Fine-Tuned Model\n",
    "\n",
    "In the cell above, you'll notice that the NDCG@10 (which was evaluated using the test set) has increased by 4-7% from the base model, showing the impact of fine-tuning!\n",
    "\n",
    "In the cell below, we show how to use the fine-tuned model to generate an embedding for a new test question. We then perform cosine similarity between the embedding and the embeddings in your corpus in order to find the most similar document to retrieve for the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1daaa96-1124-4168-99d6-f58d20db0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example showing how to generate embeddings for the fine-tuned model.\n",
    "test_embedding = model.encode(\"What manufacturing strategy does NVIDIA not employ for its products?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98061fc3-6197-4085-82b8-47649f4d185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_list = list(corpus.values())\n",
    "corpus_embeddings = model.encode(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cf037f1-6af0-492d-b7f6-a5631ee71dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_docs = semantic_search(test_embedding, corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6c531ca-6e41-4fcc-953d-a828553854b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA has a platform strategy, bringing together hardware, systems, software, algorithms, libraries, and services to create unique value for the markets we serve.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_list[similar_docs[0][0]['corpus_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda93b2-e2f5-428f-9548-7e70be37ea26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
